{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11afe06e-68bf-4d76-b359-b53475dddd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import struct\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad442530-3842-4b45-9e5b-d2a96dd2790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    # default hyperparameters for the Llama 7B model\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = 32000\n",
    "    multiple_of: int = 256  # MLP hidden layer size will be multiple of\n",
    "    norm_eps: float = 1e-5\n",
    "    max_seq_len: int = 2048\n",
    "    dropout: float = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9c0ba0-7ac3-4390-a362-6ea8815187ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54337357-d4f8-4fd9-8c36-353b8e27d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cos = torch.cos(freqs)  # real part\n",
    "    freqs_sin = torch.sin(freqs)  # imaginary part\n",
    "    return freqs_cos, freqs_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "218034c2-3f4d-48e1-b55d-75d04e33419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5749022-adde-4b68-9b19-7823625cbc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cos: torch.Tensor,\n",
    "    freqs_sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    # reshape xq and xk to match the complex representation\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "\n",
    "    # reshape freqs_cos and freqs_sin for broadcasting\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "\n",
    "    # apply rotation using real numbers\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "\n",
    "    # flatten last two dimensions\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d2903c-b98c-4ba6-a220-5c374525fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edd4068e-e0ef-4bb3-b5a1-e77e9c1a4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "        model_parallel_size = 1\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        self.dropout = args.dropout\n",
    "        \n",
    "        # use flash attention or a manual implementation\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires Pytorch >= 2.0\")\n",
    "            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float('-inf'))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        freqs_cos: torch.Tensor,\n",
    "        freqs_sin: torch.Tensor,\n",
    "    ):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        # QKV\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE relative positional embeddings\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "        # grouped multiquery attention: expand out keys and values\n",
    "        xk = repeat_kv(xk, self.n_rep) # (bs, seqlen, n_local_heads, head_dim)\n",
    "        xv = repeat_kv(xv, self.n_rep) # (bs, seqlen, n_local_heads, head_dim)\n",
    "\n",
    "        # make heads into a batch dimension\n",
    "        xq = xq.transpose(1, 2) # (bs, n_local_heads, seqlen, head_dim)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        # flash implementation\n",
    "        if self.flash:\n",
    "            output = troch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=self.dropout if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementaiton\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self, 'mask')\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]  # (bs, n_local_heads, seqlen, cache_len + seqlen)\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = torch.matmul(scores, xv) # (bs, n_local_heads, seqlen, head_dim)\n",
    "\n",
    "        # restore time as batch dimension and concat heads\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        # final projection into the residual stream\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ccd2ff4-ef0a-48b1-8b65-05c6950f979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba8bd3af-4850-4dcb-8bdf-3296abd8f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=4 * args.dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            dropout=args.dropout,\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x, freqs_cos, freqs_sin):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafb9c97-9aca-4e69-a5fa-ab9af4e0e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    last_loss: Optional[torch.Tensor]\n",
    "\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vacab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "        self.dropout = nn.Dropout(params.dropout)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        # share the unembedding parameters with the embedding parameters\n",
    "        self.tok_embeddings.weight = self.output.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # some useful precompute for the RoPE relative positional embeddings\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(self.params.dim // self.params.n_heads, self.params.max_seq_len)\n",
    "        self.register_buffer('freqs_cos', freqs_cos, persistent=False)\n",
    "        self.register_buffer('freqs_sin', freqs_sin, persistent=False)\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * params.n_layers))\n",
    "\n",
    "        # Initialize attribute for the loss of the last forward call. This will be set if the forward is called with a targets tensor.\n",
    "        self.last_loss = None\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        h = self.dropout(h)\n",
    "        freqs_cos = self.freqs_cos[:seqlen]\n",
    "        freqs_sin = self.freqs_sin[:seqlen]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cos, freqs_sin)\n",
    "        h = self.norm(h)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.output(h)\n",
    "            self.last_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the output on the very last position\n",
    "            logits = self.output(h[:, [-1], :]) # note: using list[-1] to preserve the time dim\n",
    "            self.last_loss = None\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizer(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed paramter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # create AdamW optimizer and use the fused version if it is avaialable\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {used_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the numnber of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = sum(p.numel() for p in self.parameters())\n",
    "        cfg = self.params\n",
    "        L, M, Q, T = cfg.n_layers, cfg.n_heads, cfg.dim//cfg.n_heads, cfg.max_seq_len\n",
    "        flops_per_token = 6*N + 12*L*M*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 perak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete the \n",
    "        sequence max_new_tokens times, feeding the prediction back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        Also note this is a super inefficient version of sampling with no key/value cache.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :] # crop to just the final time step\n",
    "            if temperature == 0.0:\n",
    "                # 'sample' the single most likely index\n",
    "                _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                # pluck the logits at the final step and scale by desired temperature\n",
    "                logits = logits / temperature\n",
    "                # optionally crop the logits to only the top k options\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                # apply softmax to convert logits to (normalized) probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def export(self, filepath='model.bin'):\n",
    "        \"\"\" export the model weights in fp32 into .bin file to be read from C \"\"\"\n",
    "        f = open(filepath, 'wb')\n",
    "\n",
    "        def serialize(t):\n",
    "            d = t.detach().cpu().view(-1).numpy().astype(np.float32)\n",
    "            b = struct.pack(f'{len(d)}f', *d)\n",
    "            f.write(b)\n",
    "\n",
    "        # first write out the header\n",
    "        hidden_dim = self.layers[0].feed_forward.w1.weight.shape[0]\n",
    "        p = self.params\n",
    "        n_kv_heads = p.n_heads if p.n_kv_heads is None else p.n_kv_heads\n",
    "        header = struct.pack('iiiiiii', p.dim, hidden_dim, p.n_layers, p.n_heads, n_kv_heads, p.vocab_size, p.max_seq_len)\n",
    "        f.write(header)\n",
    "\n",
    "        # next write out the embedding weights\n",
    "        serialize(self.tok_embeddings.weight)\n",
    "\n",
    "        # now all the layers\n",
    "        # attention weights\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.attention_norm.weight)\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.attention.wq.weight)\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.attention.wk.weight)\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.attention.wv.weight)\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.attention.wo.weight)\n",
    "        # ffn weights\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.ffn_norm.weight)\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.feed_forward.w1.weight)\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.feed_forward.w2.weight)\n",
    "        for layer in self.layers:\n",
    "            serialize(layer.feed_forward.w3.weight)\n",
    "        # final rmsnorm\n",
    "        serialize(self.norm.weight)\n",
    "        # note: no need to write final classifier weights due to weight sharing\n",
    "        # freqs_cis\n",
    "        serialize(self.freqs_cos[:p.max_seq_len])\n",
    "        serialize(self.freqs_sin[:p.max_seq_len])\n",
    "\n",
    "        # write to binary file\n",
    "        f.close()\n",
    "        print(f\"wrote {filepath}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9cd1e-e03c-4526-ba8b-adcd7935b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This training script can be run both on a single gpu in debug mode,\n",
    "and also in a larger training run with distributed data parallel (ddp).\n",
    "\n",
    "To run on a single GPU small debug run, example:\n",
    "$ python -m train.py --complie=False --eval_iters=10 --batch_size=8\n",
    "\n",
    "To run with DDP on 4 gpus on 1 node, example:\n",
    "$ torchrun --standalone --nproc_per_node=4 train.py\n",
    "\n",
    "To run with DDP on 4 gpus across 2 nodes, example:\n",
    "- Run on the first (master) node with example IP 123.456.123.456:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "- Run on the worker node:\n",
    "$ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
    "( If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1 )\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.distributed import destroy_process_group, init_process_group\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from tinystories import Task\n",
    "\n",
    "\n",
    "class LlaMa2:\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        self.llama = Transformer(params)\n",
    "\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        # I/O\n",
    "        self.out_dir = 'out'\n",
    "        self.eval_interal = 20 # 2000\n",
    "        self.log_interval = 1\n",
    "        self.eval_iters = 10 # 100\n",
    "        self.eval_only = False # if True, scripts exits right after the first eval\n",
    "        self.always_save_checkpoint = False # if True, always save a checkpoint after each eval\n",
    "        self.init_from = 'scratch' # 'scratch' or 'resume'\n",
    "        # wandb logging\n",
    "        self.wandb_log = False # disabled by default\n",
    "        self.wandb_project = 'llamac'\n",
    "        self.wandb_run_name = 'run' + datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n",
    "        # data\n",
    "        self.batch_size = 16 # 128 # if gradient_accumulation_step > 1, this is the micro-batch size\n",
    "        self.max_seq_len = 32 # 256\n",
    "        self.vocab_source = 'llama2' # ll2ma2|custom; use Lllama 2 vocab from Meta, or custom trained\n",
    "        self.vocab_size = 32000 # the Lllama 2 tokenizer has 32k tokens\n",
    "        # model\n",
    "        self.dim = 36 # 288\n",
    "        self.n_layers = 3 # 6\n",
    "        self.n_heads = 3 # 6\n",
    "        self.n_kv_heads = 3 # 6\n",
    "        self.multiple_of = 32\n",
    "        self.dropout = 0.0\n",
    "        # adamw optimizer\n",
    "        self.gradient_accumulation_steps = 4 # used to simulate larger batch sizes\n",
    "        self.learning_rate = 5e-4 # max learning rate\n",
    "        self.max_iters = 100 # 100000 # total number of training iterations\n",
    "        self.weight_decay = 1e-1\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.95\n",
    "        self.grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "        # learning rate decay settings\n",
    "        self.decay_lr = True # whether to decay the learning rate\n",
    "        self.warmup_iters = 1000 # how many steps to warm up for \n",
    "        # system\n",
    "        self.device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc., or try 'mps' on macbooks\n",
    "        self.dtype = 'bfloat16' # float32|bfloat16|float16\n",
    "        self.compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "        self.config_keys = [\n",
    "            k\n",
    "            for k, v in globals().items()\n",
    "            if not k.startswith('_') and isinstance(v, (int, float, bool, str))\n",
    "        ]\n",
    "        exec(open('configurator.py').read()) # override from command line or config file\n",
    "        self.config = {k: globals()[k] for k in self.config_keys} # will be useful for logging\n",
    "        # -------------------------------------------------------------------------------------------\n",
    "\n",
    "        # fixing some hyperparams to sensible defaults\n",
    "        self.lr_decay_iters = self.max_iters # should be ~= max_iters per Chinchilla\n",
    "        self.min_lr = 0.0 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "        # validating checks\n",
    "        assert self.vocab_source in ['llama2', 'custom']\n",
    "        assert self.vocab_source == 'custom' or self.vocab_size == 32000, 'The vocab from Meta has 32K tokens'\n",
    "\n",
    "        # various inits, derived attributes, I/O setup\n",
    "        self.ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "        if self.ddp:\n",
    "            init_process_group(backend='nccl')\n",
    "            self.ddp_rank = int(os.environ['RANK'])\n",
    "            self.ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "            self.ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "            self.device = f'cuda:{self.ddp_local_rank}'\n",
    "            torch.cuda.set_device(self.device)\n",
    "            self.master_process = self.ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "            self.seed_offset = self.ddp_rank # each process gets a different seed\n",
    "            # world_size number of processes will be training simultaneously, so we can scale\n",
    "            # down the desired gradient accumulation iterations per process proportionally\n",
    "            assert self.gradient_accumulation_steps % self.ddp_world_size == 0\n",
    "            self.gradient_accumulation_steps //= self.ddp_world_size\n",
    "        else:\n",
    "            # if not ddp, we are running on a single gpu, and one process\n",
    "            self.master_process = True\n",
    "            self.seed_offset = 0\n",
    "            self.ddp_world_size = 1\n",
    "        self.tokens_per_iter = self.gradient_accumulation_steps * self.ddp_world_size * self.batch_size * self.max_seq_len\n",
    "        if self.master_process:\n",
    "            print(f'tokens per iteration will be: {self.tokens_per_iter:,}')\n",
    "            print(f'breaks down as: {self.gradient_accumulation_steps} grad accum steps * {self.ddp_world_size} processes * {self.batch_size} batch size * {self.max_seq_len} max seq len')\n",
    "        \n",
    "        if self.master_process:\n",
    "            os.makedirs(self.out_dir, exist_ok=True)\n",
    "        torch.manual_seed(1337 + self.seed_offset)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "        torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "        self.device_type = 'cuda' if 'cuda' in self.device else 'cpu' # for later use in torch.autocast\n",
    "        # note: float16 data type will automatically use a GradScaler\n",
    "        self.ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[self.dtype]\n",
    "        self.ctx = {\n",
    "            nullcontext()\n",
    "            if self.device_type == 'cpu'\n",
    "            else torch.amp.autocast(device_type=self.device_type, dtype=self.ptdtype)\n",
    "        }\n",
    "\n",
    "        # task-specific setup\n",
    "        self.iter_batches = partial(\n",
    "            Task.iter_batches,\n",
    "            batch_size=self.batch_size,\n",
    "            max_seq_len=self.max_seq_len,\n",
    "            vocab_size=self.vocab_size,\n",
    "            vocab_source=self.vocab_source,\n",
    "            device=self.device,\n",
    "            num_workers=0,\n",
    "        )\n",
    "\n",
    "        # init these up here, can override if init_from='resume' (i.e. from a checkpoint)\n",
    "        self.iter_num = 0\n",
    "        self.best_val_loss = 1e9\n",
    "\n",
    "        # model init\n",
    "        self.model_args = dict(\n",
    "            dim=self.dim,\n",
    "            n_layers=self.n_layers,\n",
    "            n_heads=self.n_heads,\n",
    "            n_kv_heads=self.n_kv_heads,\n",
    "            vocab_size=self.vocab_size,\n",
    "            multiple_of=self.multiple_of,\n",
    "            max_seq_len=self.max_seq_len,\n",
    "            dropout=self.dropout,\n",
    "        ) # start with model_args from command line\n",
    "        if self.init_from == 'scratch':\n",
    "            # init a new model from scratch\n",
    "            print('Initializing a new model from scratch')\n",
    "            self.gptconf = ModelArgs(**self.model_args)\n",
    "            self.model = Transformer(self.gptconf)\n",
    "        elif self.init_from == 'resume':\n",
    "            print(f'Resuming training from {self.out_dir}')\n",
    "            # resume training from a checkpoint\n",
    "            self.ckpt_path = os.path.join(self.out_dir, 'ckpt.pt')\n",
    "            self.checkpoint = torch.load(self.ckpt_path, map_loaction=self.device)\n",
    "            self.checkpoint_model_args = self.checkpoint['model_args']\n",
    "            # force these config attributes to be equal otherwise we can't even resume training\n",
    "            # the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "            for k in ['dim', 'n_layers', 'n_heads', 'n_kv_heads', 'vocab_size', 'multiple_of', 'max_seq_len']:\n",
    "                self.model_args[k] = self.checkpoint_model_args[k]\n",
    "            # create the model\n",
    "            self.gptconf = ModelArgs(**self.model_args)\n",
    "            self.model = Transformer(**self.gptconf)\n",
    "            self.state_dict = self.checkpoint['model']\n",
    "            # fix the keys of the state dictionary\n",
    "            # honestly no idea how checkpoint sometimes get this prefix, have to debug more\n",
    "            self.unwanted_prefix = '_orig_mod.'\n",
    "            for k, v in list(state_dict.items()):\n",
    "                if k.startswith(self.unwanted_prefix):\n",
    "                    self.state_dict[k[len(self.unwanted_prefix) :]] = self.state_dict.pop(k)\n",
    "            self.model.load_state_dict(self.state_dict)\n",
    "            self.iter_num = self.checkpoint['iter_num']\n",
    "            self.best_val_loss = self.checkpoint['best_val_loss']\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # initializer a GradScaler. If enabled=False scaler is a no-op\n",
    "        self.sclaer = torch.cuda.amp.GradScaler(enabled=(self.dtype == 'float16'))\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = self.model.configure_optimizers(self.weight_decay, self.learning_rate, (self.beta1, self.beta2), self.device_type)\n",
    "        if self.init_from == 'resume' and 'optimizer' in self.checkpoint:\n",
    "            self.optimizer.load_state_dict(self.checkpoint['optimizer'])\n",
    "        self.checkpoint = None # free up memory\n",
    "\n",
    "        # compile the model\n",
    "        if self.compile:\n",
    "            print('compiling the model... (takes a ~minute)')\n",
    "            self.unoptimized_model = self.model\n",
    "            self.model = torch.compile(self.model) # requires PyTorch 2.0\n",
    "        \n",
    "        # wrap model into DDP container\n",
    "        if self.ddp:\n",
    "            # Ignore the 'freqs_cis' buffer so that DDP does not broadcast it at\n",
    "            # construction time since NCLL does not support 'ComplexFloat'\n",
    "            self.prefix = '_orig_mod.' if complie else ''\n",
    "            self.model._ddp_params_and_buffers_to_ignore = {prefix + 'freqs_cis'}\n",
    "            self.model = DDP(self.model, device_ids=[self.ddp_local_rank])\n",
    "    \n",
    "    # helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in ['train', 'val']:\n",
    "            batch_iter = self.iter_batches(split=split)\n",
    "            losses = torch.zeros(self.eval_iters) # keep on CPU\n",
    "            for k in range(self.eval_iters):\n",
    "                X,y = next(self.batch_iter)\n",
    "                with self.ctx:\n",
    "                    logits = self.model(X, y)\n",
    "                    loss = self.raw_model.last_loss\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        self.model.train()\n",
    "        return out\n",
    "    \n",
    "    # learning rate decay scheduler (cosine with warmup)\n",
    "    def get_lr(self, it):\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < self.warmup_iters:\n",
    "            return self.learning_rate * it / self.warmup_iters\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > self.lr_decay_iters:\n",
    "            return self.min_lr\n",
    "        # 3) in between, using cosine decay down to min learning rate\n",
    "        decay_ratio = (it - self.warmup_iters) / (self.lr_decay_iters - self.warmup_iters)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "        return self.min_lr + coeff * (self.learning_rate - self.min_lr)\n",
    "\n",
    "    def train(self):\n",
    "        # logging\n",
    "        if self.wandb_log and self.master_process:\n",
    "            import wandb\n",
    "            wandb.init(project=self.wandb_project, name=self.wandb_run_name, config=self.config)\n",
    "        \n",
    "        # training loop\n",
    "        train_batch_iter = iter_batches(split='train')\n",
    "        X, y = next(train_batch_iter) # fetch the very first batch\n",
    "        t0 = time.time()\n",
    "        local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "        self.raw_model = self.model.module if self.ddp else self.model # unwrap DDP container if needed\n",
    "        running_mfu = -1.0\n",
    "        while True:\n",
    "            # determine and set the learning rate for this iteration\n",
    "            lr = self.get_lr(self.iter_num) if self.decay_lr else self.learning_rate\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            # evaluate the loss on train/val sets and write checkpoints\n",
    "            if self.iter_num % self.eval_interal == 0 and self.master_process:\n",
    "                losses = self.estimate_loss()\n",
    "                print(f'step {self.iter_num}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"val\"]:.4f}')\n",
    "                if self.wandb_log:\n",
    "                    try:\n",
    "                        wandb.log(\n",
    "                            {\n",
    "                                'iter': self.iter_num,\n",
    "                                'tokens': self.iter_num * self.tokens_per_iter,\n",
    "                                'loss/train': losses['train'],\n",
    "                                'loss/val': losses['val'],\n",
    "                                'lr': lr,\n",
    "                                'mfu': running_mfu * 100, # convert to percentage\n",
    "                            }, step = self.iter_num\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f'logging to wandb failed: {e}')\n",
    "                if losses['val'] < self.best_val_loss or self.always_save_checkpoint:\n",
    "                    self.best_val_loss = losses['val']\n",
    "                    if self.iter_num > 0:\n",
    "                        self.checkpoint = {\n",
    "                            'model': self.raw_model.state_dict(),\n",
    "                            'optimizer': self.optimizer.state_dict(),\n",
    "                            'model_args': self.model_args,\n",
    "                            'iter_num': self.iter_num,\n",
    "                            'best_val_loss': self.best_val_loss,\n",
    "                            'config': self.config\n",
    "                        }\n",
    "                        print('saving checkpoint to {self.out_dir}')\n",
    "                        torch.save(self.checkpoint, os.path.join(self.out_dir, 'ckpt.pt'))\n",
    "                        model_export(self.raw_model, os.path.join(self.out_dir, 'model.bin'), version=0)\n",
    "            if self.iter_num == 0 and self.eval_only:\n",
    "                break\n",
    "\n",
    "            # forward backward update, with optional gradient accumlation to simulate larger batch size\n",
    "            # and using the GradScaler if data type is float16\n",
    "            for micro_step in range(self.gradient_accumulation_steps):\n",
    "                if self.ddp:\n",
    "                    # in DDP training we only need to sync gradients at the last micro step.\n",
    "                    # the official way to do this is with model.no_sync() context manager, but\n",
    "                    # I really dislike that this bloats the code and forces us to repeat code\n",
    "                    # looking at the source of the context manager, it just toggles this variable \n",
    "                    self.model.require_backward_grad_sync = micro_step == self.gradient_accumulation_steps - 1\n",
    "                with self.ctx:\n",
    "                    logits = self.model(X, y)\n",
    "                    loss = self.raw_model.last_loss\n",
    "                    loss = loss / self.gradient_accumulation_steps\n",
    "                # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "                X, y = next(train_batch_iter)\n",
    "                # backward pass, with gradient scaling if training in fp16\n",
    "                self.scaler.scale(loss).backward()\n",
    "            # clip the gradient\n",
    "            if self.grad_clip != 0.0:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "            # step the optimizer and scaler if training in fp16\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            # flush the gradients as soon as we can, no need for this memory anymore\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # timing and logging\n",
    "            t1 = time.time()\n",
    "            dt = t1 - t0\n",
    "            t0 = t1\n",
    "            if self.iter_num % self.log_interval == 0 and self.master_process:\n",
    "                # get loss and float, scale up due to the divide above. note: this is a CPU-GPU sync point\n",
    "                lossf = loss.item() * self.gradient_accumulation_steps\n",
    "                if self.local_iter_num >= 5: # let the training loop settle a bit\n",
    "                    mfu = self.raw_model.estimate_mfu(self.batch_size * self.gradient_accumulation_steps, dt)\n",
    "                    running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "                print(f'{self.iter_num} | loss {lossf:.4f} | lr {lr:e} | {dt*1000:.2f}ms | mfu {running_mfu*100:.2f}%')\n",
    "            self.iter_num += 1\n",
    "            self.local_iter_num += 1\n",
    "\n",
    "            # termination conditions\n",
    "            if self.iter_num > self.max_iters:\n",
    "                break\n",
    "        \n",
    "        if self.ddp:\n",
    "            destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c2d03-badc-494d-ae1c-a849323af98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LlaMa2(ModelArgs()).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0d4f5-fb91-4f30-a383-8e4ca24f7d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902769b-697b-4ad1-8015-d6666ad262c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
