{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "976ddae0-e2d4-4173-bcff-7e180dd5c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This prediction script is the Python version of run.c\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from model import Transformer, ModelArgs\n",
    "from tokenizer import Tokenizer\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa9f3931-797b-4935-840f-dd42b10d5735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('stories15M.pt', map_location='cpu')\n",
    "model_args = checkpoint[\"model_args\"]\n",
    "# force these config attributes to be equal otherwise we can't even resume training\n",
    "# the rest of the attributes (e.g. dropout) can stay as desired from command line\n",
    "# create the model\n",
    "gptconf = ModelArgs(**model_args)\n",
    "model = Transformer(gptconf)\n",
    "state_dict = checkpoint[\"model\"]\n",
    "# fix the keys of the state dictionary :(\n",
    "# honestly no idea how checkpoints sometimes get this prefix, have to debug more\n",
    "unwanted_prefix = \"_orig_mod.\"\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix) :]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b976027b-c21a-435e-9cea-0c259a7621ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "acfbe2cd-d84b-4f46-b5af-0ba18ae2c39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = model.params.max_seq_len\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4be4ae7-3f17-434d-8b39-e9d28037d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer('tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "73f564de-a0db-4c26-9857-5b52b7c37d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''\n",
    "steps = 1024\n",
    "temperature = 0.0   # 0.0 = greedy deterministic. 1.0 = original. don't set higher\n",
    "topp = 1.0         # top-p in nucleus sampling. 1.0 = off. 0.9 works well, but slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "54f7bf95-9e35-4f24-8c4d-06aa6ca61b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_prompt_tokens = 0\n",
    "prompt_tokens = []\n",
    "if prompt:\n",
    "    prompt_tokens = tokenizer.encode(prompt, False, False)\n",
    "    num_prompt_tokens = len(prompt_tokens)\n",
    "prompt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c71a3d8e-31a8-4ab8-b346-87866aee2921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, red ball in the sky. It was the sun! She thought it was so pretty.\n",
      "Lily wanted to play with the ball, but it was too high up in the sky. She tried to jump and reach it, but she couldn't. Then, she had an idea. She would use a stick to knock the ball down.\n",
      "Lily found a stick and tried to hit the ball. But the stick was too short. She tried again and again, but she couldn't reach it. She felt sad.\n",
      "Suddenly, a kind man came by and saw Lily. He asked her what was wrong. Lily told him about the ball. The man smiled and said, \"I have a useful idea!\" He took out a long stick and used it to knock the ball down. Lily was so happy! She thanked the man and they played together in the sunshine.\n",
      "\n",
      "\n",
      "achieved tok/s: 27.625000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BOS = 1\n",
    "# start the main loop\n",
    "start = 0   # used to time our code, only initialized after first iteration\n",
    "nxt = 0     # will store the next token in the sequence\n",
    "token = BOS # init with token 1 (=BOS), as done in Llama-2 sentencepiece tokenizer\n",
    "pos = 0     # position in the sequence\n",
    "token_str = ''\n",
    "tokens = []\n",
    "X = torch.tensor([[token]], dtype=torch.long)\n",
    "\n",
    "while pos < steps:\n",
    "    logits = model(X)\n",
    "    if pos < num_prompt_tokens:\n",
    "        nxt = prompt_tokens[pos]\n",
    "    else:\n",
    "        if temperature == 0.0:\n",
    "            nxt = torch.argmax(logits).item()\n",
    "        else:\n",
    "            # focus only on the last time step, pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature # becomes (B, C)\n",
    "            if 0 < topp < 1:\n",
    "                top_k = int(logits.shape[-1] * (1-topp))\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            nxt = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            nxt = nxt.item()\n",
    "    pos += 1\n",
    "\n",
    "    # data-dependent terminating condition: the BOS (1) token delimits sequences\n",
    "    if nxt == BOS:\n",
    "        break\n",
    "\n",
    "    # init the timer here because the first iteration can be slower\n",
    "    if start == 0:\n",
    "        start = datetime.now()\n",
    "    # following BOS (1) token, sentencepiece decoder strips any leading whitespace (see PR #89)\n",
    "    tokens.append(nxt)\n",
    "    token_str = tokenizer.decode(tokens)\n",
    "    # print(f\"{token_str}\", end='\\r', flush=True)\n",
    "    # print(\"\\r{}\".format(token_str), end=\"\")\n",
    "    # sys.stdout.write('\\r' + token_str)\n",
    "    # time.sleep(0.05) # This line is to see if it's working or not\n",
    "    # Clear the previous output\n",
    "    clear_output(wait=True)\n",
    "    print(token_str)\n",
    "    # append sampled index to the running sequence\n",
    "    X = torch.cat((X, torch.tensor([[nxt]], dtype=torch.long)), dim=1) # (B, T+1)\n",
    "    # if the sequence context is growing too long we must crop it at block_size\n",
    "    X = X if X.size(-1) <= max_seq_len else X[:, -max_seq_len:]\n",
    "            \n",
    "\n",
    "# report achieved tok/s (pos-1 because the timer starts after first iteration)\n",
    "if pos > 1:\n",
    "    end = datetime.now()\n",
    "    print(f\"\\n\\nachieved tok/s: {(pos-1)/(end-start).seconds:6f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b23809-1472-46b1-9e8c-b7c84d9f4b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
